{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c4a7798-9025-49c7-9453-589096c8b1a2",
   "metadata": {},
   "source": [
    "dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c131904-e8fe-4eb7-bf91-e524088d632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum label counts across datasets: {0: 54, 1: 36, 2: 27, 3: 36, 4: 23}\n",
      "superchris: Label counts = {0: 54, 1: 36, 2: 27, 3: 36, 4: 23}\n",
      "barat: Label counts = {0: 54, 1: 36, 2: 27, 3: 36, 4: 23}\n",
      "stella: Label counts = {0: 54, 1: 36, 2: 27, 3: 36, 4: 23}\n",
      "mitt: Label counts = {0: 54, 1: 36, 2: 27, 3: 36, 4: 23}\n",
      "buchanan: Label counts = {0: 54, 1: 36, 2: 27, 3: 36, 4: 23}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_max_pool\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.nn import dense_diff_pool\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "def load_raw_data(dir_data, rat_name): \n",
    "    trlPath = os.path.join(dir_data, rat_name, rat_name.lower() + '_trial_info.npy') \n",
    "    spkPath = os.path.join(dir_data, rat_name, rat_name.lower() + '_spike_data_binned.npy') \n",
    "    lfpPath = os.path.join(dir_data, rat_name, rat_name.lower() + '_lfp_data_sampled.npy')\n",
    "    \n",
    "    trial_info = np.load(trlPath) \n",
    "    spike_data = np.load(spkPath)\n",
    "    lfp_data = np.load(lfpPath)\n",
    "    lfp_data = np.swapaxes(lfp_data, 1, 2) \n",
    "    return trial_info, spike_data, lfp_data\n",
    "\n",
    "\n",
    "\n",
    "trial_info_superchris, spike_data_superchris, lfp_data_superchris = load_raw_data(dir_data=\"epoch_data\", rat_name=\"superchris\")\n",
    "trial_info_barat, spike_data_barat, lfp_data_barat = load_raw_data(dir_data=\"epoch_data\", rat_name=\"barat\")\n",
    "trial_info_stella, spike_data_stella, lfp_data_stella = load_raw_data(dir_data=\"epoch_data\", rat_name=\"stella\")\n",
    "trial_info_mitt, spike_data_mitt, lfp_data_mitt = load_raw_data(dir_data=\"epoch_data\", rat_name=\"mitt\")\n",
    "trial_info_buchanan, spike_data_buchanan, lfp_data_buchanan = load_raw_data(dir_data=\"epoch_data\", rat_name=\"buchanan\")\n",
    "\n",
    "\n",
    "# Assuming the files are correctly loaded, the focus will be on processing the data.\n",
    "\n",
    "# Update count_labels to count occurrences of all unique labels\n",
    "def count_labels(trial_info, target_col=3):\n",
    "    labels = trial_info[:, target_col] - 1  # Adjust for target variable\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    label_counts = dict(zip(unique, counts))\n",
    "    return label_counts\n",
    "\n",
    "# Count labels for each rat dataset\n",
    "counts = {\n",
    "    \"superchris\": count_labels(trial_info_superchris),\n",
    "    \"barat\": count_labels(trial_info_barat),\n",
    "    \"stella\": count_labels(trial_info_stella),\n",
    "    \"mitt\": count_labels(trial_info_mitt),\n",
    "    \"buchanan\": count_labels(trial_info_buchanan),\n",
    "}\n",
    "\n",
    "# Find the minimum count for each label across datasets\n",
    "min_label_counts = {}\n",
    "for label in range(5):  # Adjust for labels 0-4\n",
    "    min_label_counts[label] = min(counts[rat_name].get(label, 0) for rat_name in counts.keys())\n",
    "\n",
    "print(\"Minimum label counts across datasets:\", min_label_counts)\n",
    "\n",
    "# Update select_balanced_trials to balance across all labels\n",
    "def select_balanced_trials(trial_info, spike_data, lfp_data, target_col=3, n_labels=None):\n",
    "    if n_labels is None:\n",
    "        n_labels = {label: 10 for label in range(5)}  # Default to 10 trials per label\n",
    "\n",
    "    labels = trial_info[:, target_col] - 1  # Adjust for target variable\n",
    "    selected_indices = []\n",
    "\n",
    "    for label, n_label in n_labels.items():\n",
    "        # Get indices of trials with the current label\n",
    "        indices_label = np.where(labels == label)[0]\n",
    "\n",
    "        # Handle cases where available trials are less than requested\n",
    "        n_label = min(len(indices_label), n_label)\n",
    "\n",
    "        # Randomly select required number of trials for the current label\n",
    "        selected_indices_label = np.random.choice(indices_label, n_label, replace=False)\n",
    "        selected_indices.extend(selected_indices_label)\n",
    "\n",
    "    # Sort selected indices\n",
    "    selected_indices = np.sort(selected_indices)\n",
    "\n",
    "    # Subset the trial_info, spike_data, and lfp_data based on selected indices\n",
    "    trial_info_balanced = trial_info[selected_indices]\n",
    "    spike_data_balanced = spike_data[selected_indices]\n",
    "    lfp_data_balanced = lfp_data[selected_indices]\n",
    "\n",
    "    return trial_info_balanced, spike_data_balanced, lfp_data_balanced\n",
    "\n",
    "# Apply this function to all datasets independently\n",
    "balanced_datasets = {}\n",
    "for rat_name, trial_info, spike_data, lfp_data in [\n",
    "    (\"superchris\", trial_info_superchris, spike_data_superchris, lfp_data_superchris),\n",
    "    (\"barat\", trial_info_barat, spike_data_barat, lfp_data_barat),\n",
    "    (\"stella\", trial_info_stella, spike_data_stella, lfp_data_stella),\n",
    "    (\"mitt\", trial_info_mitt, spike_data_mitt, lfp_data_mitt),\n",
    "    (\"buchanan\", trial_info_buchanan, spike_data_buchanan, lfp_data_buchanan),\n",
    "]:\n",
    "    trial_info_balanced, spike_data_balanced, lfp_data_balanced = select_balanced_trials(\n",
    "        trial_info, spike_data, lfp_data, n_labels=min_label_counts\n",
    "    )\n",
    "    balanced_datasets[rat_name] = {\n",
    "        \"trial_info\": trial_info_balanced,\n",
    "        \"spike_data\": spike_data_balanced,\n",
    "        \"lfp_data\": lfp_data_balanced,\n",
    "    }\n",
    "\n",
    "# Check the balanced datasets\n",
    "for rat_name, data in balanced_datasets.items():\n",
    "    trial_info_balanced = data[\"trial_info\"]\n",
    "    labels_balanced = trial_info_balanced[:, 3] - 1\n",
    "    print(f\"{rat_name}: Label counts = {dict(zip(*np.unique(labels_balanced, return_counts=True)))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85edeeba-ef7b-4a8e-91cf-071216079b49",
   "metadata": {},
   "source": [
    "MGMT+MAGNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99732877-001b-486b-962e-1a3929aa6f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-21 17:57:46,351] A new study created in memory with name: no-name-779e2fd4-016b-44f0-b2d0-9d6389697342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 1 - Time: 2.5522 seconds\n",
      "Fold 1:\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 0., 0.],\n",
      "        [1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 0., 0.],\n",
      "        [1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 0., 0.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 0., 0.],\n",
      "        [1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 0., 0.],\n",
      "        [1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 0., 0.],\n",
      "        [1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 0., 0.],\n",
      "        [1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "         1., 0., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 0.]])\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 1.],\n",
      "        [1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "         1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "         1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "         0., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 0., 1.],\n",
      "        [1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 0.]])\n",
      "tensor([[0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 1.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
      "         0., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
      "         0., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         1., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 0.]])\n",
      "tensor([[0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1.],\n",
      "        [0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 1.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
      "         0., 1.],\n",
      "        [1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0.],\n",
      "        [0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
      "         0., 1.],\n",
      "        [0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         0., 1.],\n",
      "        [0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "         0., 1.],\n",
      "        [0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "         1., 1.],\n",
      "        [0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1.],\n",
      "        [0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0.]])\n",
      "weakfirst time 8.3088 seconds\n",
      "weakfirst time 11.5113 seconds\n",
      "weakfirst time 12.9227 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-21 18:02:22,425] Trial 0 failed with parameters: {'layer_num': 3, 'threshold1': 0.1, 'batch_size': 32, 'num_train': 40, 'num_epochs': 10, 'dropout1': 0.25, 'lr': 0.15957993164212958, 'train_weak': 5, 'train_middle': 40, 'weight_decay_weak': 0.0003976453011700043, 'lr_weak': 0.05854751355295725, 'weight_decay_middle': 8.841926348917726e-05, 'lr_middle': 0.05571905096939266, 'batch1': 8} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zmoslemi/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_2276309/1290689301.py\", line 810, in objective\n",
      "    train_acc = model_acc(model, training[i], edge_indices[i], training_labels[i])\n",
      "  File \"/tmp/ipykernel_2276309/1290689301.py\", line 118, in model_acc\n",
      "    output_model = model(data.x, data.edge_index, batch=data.batch)[0]  # Extract logits\n",
      "  File \"/home/zmoslemi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zmoslemi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_2276309/1290689301.py\", line 262, in forward\n",
      "    x, attention1 = self.conv1(x, edge_index, return_attention_weights=True)\n",
      "  File \"/home/zmoslemi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zmoslemi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/zmoslemi/.local/lib/python3.10/site-packages/torch_geometric/nn/conv/transformer_conv.py\", line 223, in forward\n",
      "    query = self.lin_query(x[1]).view(-1, H, C)\n",
      "  File \"/home/zmoslemi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/zmoslemi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/zmoslemi/.local/lib/python3.10/site-packages/torch_geometric/nn/dense/linear.py\", line 147, in forward\n",
      "    return F.linear(x, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "[W 2025-04-21 18:02:22,430] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2276309/1290689301.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;31m# Run Optuna optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;31m# Best trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2276309/1290689301.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    808\u001b[0m                     \u001b[0mtraining_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m                     \u001b[0mcurr_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss_compute\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m                 \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2276309/1290689301.py\u001b[0m in \u001b[0;36mmodel_acc\u001b[0;34m(model, dataset, edge_index, labels)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0moutput_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Extract logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# Get predicted class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2276309/1290689301.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, return_node_embeddings, batch)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_node_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_attention_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mlap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/conv/transformer_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/dense/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \"\"\"\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np \n",
    "import os\n",
    "import random \n",
    "from torch_geometric.data import Data\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv, GATConv, APPNP, TransformerConv\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, Dropout\n",
    "from custom_pooling import global_max_pool\n",
    "from convlayer import MaGNetConv\n",
    "from norm_operation import PairNorm, MeanSubtractionNorm\n",
    "from gpslayer import * \n",
    "import warnings\n",
    "from itertools import product\n",
    "np.random.seed(1) # mitt\n",
    "torch.manual_seed(1)\n",
    "import torch\n",
    "torch.use_deterministic_algorithms(False)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from custom_pooling import global_max_pool\n",
    "import os\n",
    "import torch\n",
    "import optuna\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_max_pool, dense_diff_pool, global_mean_pool, TransformerConv, BatchNorm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from custom_pooling import global_max_pool\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def generate_hyperparam_combinations(hyperparameters):\n",
    "    keys, values = zip(*hyperparameters.items())\n",
    "    return [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "\n",
    "def prepare_gnn_dataset(lfp_data, trial_info, target_col=3, corr_threshold=0.5):\n",
    "    dataset = []\n",
    "    for i in range(lfp_data.shape[0]):\n",
    "        node_features = torch.tensor(lfp_data[i], dtype=torch.float)\n",
    "        edge_index = create_edge_index_from_correlation(node_features, threshold=corr_threshold)\n",
    "        y = torch.tensor(trial_info[i, target_col] - 1, dtype=torch.long)\n",
    "        data = Data(x=node_features, edge_index=edge_index, y=y)\n",
    "        dataset.append(data)\n",
    "    return dataset\n",
    "\n",
    "def compute_loss(model, dataset, edge_index):\n",
    "    out, feat, emb = [], [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_data = torch.tensor(dataset.transpose(0, 2, 1), dtype=torch.float, device=device)  \n",
    "        edge_index = edge_index.to(device)\n",
    "\n",
    "        for i in range(x_data.shape[0]):  \n",
    "            batch = torch.zeros(x_data[i].shape[0], dtype=torch.long, device=device)  # Assign nodes to batch\n",
    "            data = Data(x=x_data[i], edge_index=edge_index, batch=batch)\n",
    "\n",
    "            output_model = model(data.x, data.edge_index, batch=data.batch)  \n",
    "\n",
    "            out.append(output_model[0])  # Logits\n",
    "            feat.append(output_model[1])  \n",
    "\n",
    "            # ✅ Ensure emb is a tensor, not a tuple\n",
    "            emb_val = output_model[2]\n",
    "            if isinstance(emb_val, tuple):\n",
    "                emb_val = torch.stack(emb_val, dim=0)  # Convert tuple to tensor\n",
    "            emb.append(emb_val)  \n",
    "\n",
    "    soft_prob = torch.cat(out, dim=0)  \n",
    "    feat = torch.stack([f.T for f in feat], dim=0)  \n",
    "\n",
    "    # ✅ Fix: Ensure emb is stacked properly\n",
    "    try:\n",
    "        emb = torch.stack(emb, dim=0)\n",
    "    except TypeError as e:\n",
    "        print(f\"⚠️ TypeError in stacking emb: {e}\")\n",
    "        print(f\"Emb element types: {[type(e) for e in emb]}\")\n",
    "        emb = torch.cat([e.unsqueeze(0) if e.dim() < 3 else e for e in emb], dim=0)\n",
    "\n",
    "    return [soft_prob, feat, emb]\n",
    "\n",
    "\n",
    "def model_acc(model, dataset, edge_index, labels):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_data = torch.tensor(dataset.transpose(0, 2, 1), dtype=torch.float, device=device)  \n",
    "        edge_index = edge_index.to(device)\n",
    "\n",
    "        for i in range(x_data.shape[0]):  \n",
    "            batch = torch.zeros(x_data[i].shape[0], dtype=torch.long, device=device)  # Assign nodes to batch\n",
    "            data = Data(x=x_data[i], edge_index=edge_index, batch=batch)\n",
    "\n",
    "            # Forward pass\n",
    "            output_model = model(data.x, data.edge_index, batch=data.batch)[0]  # Extract logits\n",
    "\n",
    "            # Get predicted class\n",
    "            pred_label = output_model.argmax(dim=1)\n",
    "            predictions.append(pred_label[0])\n",
    "\n",
    "    # Convert predictions to tensor\n",
    "    pred_tensor = torch.tensor(predictions, dtype=torch.long, device=device)\n",
    "\n",
    "    # Compute accuracy efficiently\n",
    "    correct = (pred_tensor == labels).sum().item()\n",
    "    accuracy = correct / dataset.shape[0]\n",
    "\n",
    "    return accuracy\n",
    "   \n",
    "   \n",
    "\n",
    "# Compute the weighted loss error rate\n",
    "def weight_loss(model, dataset, weights, edge_index, training_label):\n",
    "    pred = []\n",
    "\n",
    "    # model prediction on graph labels\n",
    "    for i in range(dataset.shape[0]):\n",
    "        nodes_fea = [tmp for tmp in np.transpose(dataset[i,:,:])]\n",
    "        x = torch.tensor(nodes_fea, dtype=torch.float)\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        data = data.to(device)\n",
    "        pred.append(model(data.x, data.edge_index)[0].argmax(dim=1)[0])\n",
    "    pred = torch.Tensor(pred).to(device)\n",
    "    \n",
    "    # compute the weighted error rate for classifier \n",
    "    err_rate = (weights.to(device)*(pred != training_label)).sum()/weights.sum()\n",
    "\n",
    "    return err_rate\n",
    "\n",
    "# Evaluate the stabilized quality of the classifier \n",
    "def quality_update(err_rate):\n",
    "    # negative logit function\n",
    "    alpha = torch.log(0.01 + err_rate / (1 - err_rate))\n",
    "    return torch.max(-alpha, torch.tensor(0.05))\n",
    "\n",
    "# Update the weights for classifiers \n",
    "def weight_update(model, err_rate, dataset, weights, edge_index, training_label):\n",
    "    alpha = quality_update(err_rate)\n",
    "\n",
    "    for i in range(dataset.shape[0]):\n",
    "        nodes_fea = [tmp for tmp in np.transpose(dataset[i,:,:])]\n",
    "        x = torch.tensor(nodes_fea, dtype=torch.float)\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        data = data.to(device)\n",
    "\n",
    "        # model prediction and weights updating via exponential rule\n",
    "        curr_pred = torch.Tensor(model(data.x, data.edge_index)[0].argmax(dim=1)[0]).to(device) \n",
    "        weights[i] = torch.exp(alpha * (training_label[i] != curr_pred))\n",
    "\n",
    "    # normalize the weights for all samples\n",
    "    weights = nn.functional.normalize(weights, p=2, dim=0)\n",
    "        \n",
    "    return weights\n",
    "\n",
    "    # Evaluate the final model performance \n",
    "def test_acc_and_confusion_matrix(model):\n",
    "    predictions = []\n",
    "\n",
    "    # build evaluation dataset and batch \n",
    "    X_test_tensor = test_emb.clone().detach()\n",
    "    test_dataset = TensorDataset(X_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "    \n",
    "    # freeze the model parameters and perform inference \n",
    "    with torch.no_grad():\n",
    "        for features in test_loader:\n",
    "            features = features[0]  # Unpack the single-element tuple\n",
    "            outputs = model(features)\n",
    "            predicted = outputs.argmax(dim=1)  # Get the class with the highest probability\n",
    "            predictions.extend(predicted.tolist())\n",
    "\n",
    "    # calculate the accuracy\n",
    "    accuracy = accuracy_score(testing_label.cpu(), predictions)\n",
    "    \n",
    "    # calculate the confusion matrix\n",
    "    cm = confusion_matrix(testing_label.cpu(), predictions)\n",
    "\n",
    "    return accuracy, cm\n",
    "\n",
    "class Weaker_First(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, num_node_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes  # Store number of nodes\n",
    "        self.conv0 = nn.Conv1d(num_nodes, num_nodes, 3, 1, padding=1)\n",
    "        self.conv1 = TransformerConv(num_node_features, 32, heads=4, concat=True, dropout=0.1)\n",
    "        self.conv2 = TransformerConv(32 * 4, 128, heads=4, concat=True, dropout=0.1)\n",
    "        self.conv3 = TransformerConv(128 * 4, 32, heads=4, concat=True, dropout=0.1)\n",
    "\n",
    "        self.local_size = min(7, num_nodes)  # Ensure valid local size\n",
    "        self.final_feature_dim = 32 * 4  # Ensure correct output size\n",
    "\n",
    "        self.lr1 = nn.Linear(self.final_feature_dim, 128)\n",
    "        self.lr2 = nn.Linear(128, 32)\n",
    "        self.lr3 = nn.Linear(32, num_classes)\n",
    "        self.attention_scores = None\n",
    "        \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x = self.conv0(x)\n",
    "        x, attn1 = self.conv1(x, edge_index, return_attention_weights=True)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        x, attn2 = self.conv2(x, edge_index, return_attention_weights=True)\n",
    "        x = F.relu(x)\n",
    "        x, attn3 = self.conv3(x, edge_index, return_attention_weights=True)\n",
    "        x = F.relu(x)\n",
    "        lap = x  # Store intermediate representation\n",
    "\n",
    "        if batch is None:\n",
    "            batch = torch.zeros(x.shape[0], dtype=torch.long, device=x.device)\n",
    "\n",
    "        x = global_max_pool(x, batch)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.lr1(x))\n",
    "        x = F.relu(self.lr2(x))\n",
    "        x = self.lr3(x)\n",
    "        \n",
    "        self.attention_scores = attn1[1] + attn2[1] + attn3[1]\n",
    "        return [F.log_softmax(x, dim=1), lap, x]\n",
    "\n",
    "class Weaker_Middle(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, heads=4, dropout=0.1):  \n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim  # Dynamically set input feature size\n",
    "        self.num_classes = num_classes  \n",
    "\n",
    "        self.conv1 = TransformerConv(128, 32, heads=heads, concat=True, dropout=dropout)  \n",
    "        \n",
    "        self.hidden_dim = 32 * heads  # Ensure proper feature size\n",
    "        self.fc_input_dim = self.hidden_dim  # Update based on actual feature size\n",
    "        \n",
    "        self.lr1 = nn.Linear(self.fc_input_dim, 128)\n",
    "        self.lr2 = nn.Linear(128, 48)\n",
    "        self.lr3 = nn.Linear(48, self.num_classes)  \n",
    "\n",
    "        self.attention_scores = None  \n",
    "\n",
    "    def forward(self, x, edge_index, return_node_embeddings=False, batch=None):\n",
    "        x, attention1 = self.conv1(x, edge_index, return_attention_weights=True)  \n",
    "        self.attention_scores = attention1[1]  \n",
    "        lap = x  \n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        if batch is None:\n",
    "            batch = torch.zeros(x.shape[0], dtype=torch.long, device=x.device)\n",
    "\n",
    "        # Pooling over nodes\n",
    "        x = global_max_pool(x, batch)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.lr1(x))\n",
    "        x = F.relu(self.lr2(x))\n",
    "        x = self.lr3(x)\n",
    "\n",
    "        if return_node_embeddings:\n",
    "            return x  \n",
    "\n",
    "        return [F.log_softmax(x, dim=1), lap, x]\n",
    "\n",
    "\n",
    "\n",
    "def normalize_features(train_data, test_data, val_data):\n",
    "    \"\"\"\n",
    "    Normalizes the LFP dataset using StandardScaler, ensuring data type remains unchanged.\n",
    "    \"\"\"\n",
    "    train_features = train_data.reshape(-1, train_data.shape[-1])  # Flatten across samples\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_features)  # Fit only on training data\n",
    "\n",
    "    # Apply transformation without changing data type\n",
    "    train_data = scaler.transform(train_data.reshape(-1, train_data.shape[-1])).reshape(train_data.shape)\n",
    "    test_data = scaler.transform(test_data.reshape(-1, test_data.shape[-1])).reshape(test_data.shape)\n",
    "    val_data = scaler.transform(val_data.reshape(-1, val_data.shape[-1])).reshape(val_data.shape)\n",
    "\n",
    "    return train_data, test_data, val_data\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def set_seed(seed=1):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "set_seed(1)\n",
    "\n",
    "\n",
    "class GraphLevelPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=5, heads=4, dropout=0.1):\n",
    "        super(GraphLevelPredictor, self).__init__()\n",
    "        self.conv1 = TransformerConv(input_dim, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.conv2 = TransformerConv(hidden_dim * heads, hidden_dim, heads=heads, dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * heads, output_dim)  \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "\n",
    "        x = global_mean_pool(x, batch)  # Aggregates node embeddings into graph embeddings\n",
    "\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def create_edge_index_from_correlation(data, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Create an edge index based on the correlation matrix of the data.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): A 2D tensor of shape (num_nodes, num_features), where rows are nodes and columns are features.\n",
    "        threshold (float): Correlation threshold. Only edges with |correlation| > threshold are included.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Edge index tensor of shape [2, num_edges].\n",
    "    \"\"\"\n",
    "    data_np = data.numpy()  # Convert to NumPy for correlation computation\n",
    "    num_nodes = data_np.shape[0]  # Number of nodes\n",
    "    correlation_matrix = np.corrcoef(data_np.T)  # Compute correlation between features\n",
    "    \n",
    "    # Get indices where |correlation| > threshold\n",
    "    indices = np.where(np.abs(correlation_matrix) > threshold)\n",
    "    \n",
    "    # Remove self-loops (correlations between the same nodes)\n",
    "    valid_edges = indices[0] != indices[1]\n",
    "    source_nodes = indices[0][valid_edges]\n",
    "    target_nodes = indices[1][valid_edges]\n",
    "    \n",
    "    # Ensure indices are within valid range\n",
    "    valid_mask = (source_nodes < num_nodes) & (target_nodes < num_nodes)\n",
    "    source_nodes = source_nodes[valid_mask]\n",
    "    target_nodes = target_nodes[valid_mask]\n",
    "    \n",
    "    # Create edge index tensor\n",
    "    edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "def prepare_gtn_dataset(lfp_data, trial_info, target_col=3, corr_threshold=0.5):\n",
    "    dataset = []\n",
    "    for i in range(lfp_data.shape[0]):\n",
    "        node_features = torch.tensor(lfp_data[i], dtype=torch.float)\n",
    "        edge_index = create_edge_index_from_correlation(node_features, threshold=corr_threshold)\n",
    "        y = torch.tensor(trial_info[i, target_col] - 1, dtype=torch.long)\n",
    "        data = Data(x=node_features, edge_index=edge_index, y=y)\n",
    "        dataset.append(data)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def set_weights(model):\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Linear) or isinstance(layer, nn.Conv2d):\n",
    "            torch.manual_seed(1)\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.zeros_(layer.bias)\n",
    " \n",
    "class GTN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, num_classes, heads=4, dropout=0.1):\n",
    "        super(GTN, self).__init__()\n",
    "        self.conv1 = TransformerConv(num_node_features, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.conv2 = TransformerConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.conv3 = TransformerConv(hidden_channels * heads, 32, heads=1, dropout=dropout)\n",
    "\n",
    "        self.batch_norm = nn.LayerNorm(32)  \n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.attention_scores = None\n",
    "        \n",
    "    def forward(self, data, return_node_embeddings=False):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x, attention1 = self.conv1(x, edge_index, return_attention_weights=True)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x) if self.training else x\n",
    "        x, attention2 = self.conv2(x, edge_index, return_attention_weights=True)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x) if self.training else x\n",
    "        x, attention3 = self.conv3(x, edge_index, return_attention_weights=True)\n",
    "        x = F.relu(x)\n",
    "        x = self.batch_norm(x)  \n",
    "\n",
    "        if return_node_embeddings:\n",
    "            return x  # Return per-node embeddings\n",
    "\n",
    "        # Aggregate node embeddings into graph-level representation\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        self.attention_scores = attention1[1] + attention2[1] + attention3[1]\n",
    "        \n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "def train_gtns(loaders, gtn_models, epochs=5, lr=0.002):\n",
    "    for gtn in gtn_models:\n",
    "        gtn.to(device)\n",
    "\n",
    "    optimizers = [torch.optim.Adam(gtn.parameters(), lr=lr) for gtn in gtn_models]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for loader, gtn, optimizer in zip(loaders, gtn_models, optimizers):\n",
    "            for batch in loader:\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Use graph-level representation\n",
    "                out = gtn(batch, return_node_embeddings=False)\n",
    "\n",
    "                target = batch.y.view(-1).long()  # Ensure correct shape\n",
    "                loss = criterion(out, target)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "        print(f\"GTN Training Epoch {epoch + 1}, Loss: {total_loss / len(loaders):.4f}\")\n",
    "\n",
    "\n",
    "def extract_features(dataset, gtn_model):\n",
    "    encoded_features = []\n",
    "    gtn_model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            data = data.to(device)\n",
    "            node_embeddings = gtn_model(data, return_node_embeddings=True)  # Extract node-level features\n",
    "            encoded_features.append(Data(x=node_embeddings, edge_index=data.edge_index, y=data.y))\n",
    "\n",
    "    return encoded_features\n",
    "\n",
    "def compute_node_attention_scores(attention_scores, edge_index, num_nodes):\n",
    "    \"\"\"\n",
    "    Compute node-level weights using extracted multi-head attention scores.\n",
    "\n",
    "    Args:\n",
    "        attention_scores (torch.Tensor): Multi-head edge attention scores of shape [num_edges, num_heads].\n",
    "        edge_index (torch.Tensor): Edge indices of shape [2, num_edges].\n",
    "        num_nodes (int): Number of nodes in the graph.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Node attention weights of shape [num_nodes].\n",
    "    \"\"\"\n",
    "\n",
    "    num_edges = min(edge_index.shape[1], attention_scores.shape[0])  # Ensure valid indexing\n",
    "    edge_index = edge_index[:, :num_edges]  # Truncate excess edges\n",
    "    attention_scores = attention_scores[:num_edges]  # Truncate excess scores\n",
    "\n",
    "    node_scores = torch.zeros(num_nodes, device=attention_scores.device)\n",
    "\n",
    "    # Aggregate attention scores across heads\n",
    "    attention_scores = attention_scores.mean(dim=1)  # Shape: [num_edges]\n",
    "\n",
    "    # Ensure edges don't reference nodes outside valid range\n",
    "    valid_mask = (edge_index[0] < num_nodes) & (edge_index[1] < num_nodes)\n",
    "    edge_index = edge_index[:, valid_mask]  # Remove invalid edges\n",
    "    attention_scores = attention_scores[valid_mask]  # Adjust attention scores\n",
    "\n",
    "    # Aggregate attention scores per node\n",
    "    for i, (src, dst) in enumerate(edge_index.t()):\n",
    "        node_scores[src] += attention_scores[i].item()\n",
    "        node_scores[dst] += attention_scores[i].item()\n",
    "\n",
    "    return node_scores\n",
    "\n",
    "\n",
    "def construct_subgraph(graph, significant_nodes, edge_weight_threshold=0.5):\n",
    "    node_indices = significant_nodes.nonzero(as_tuple=True)[0]  # Get selected node indices\n",
    "    graph.x = graph.x.T\n",
    "    \n",
    "    if node_indices.numel() == 0:\n",
    "        print(\"⚠️ No significant nodes found. Selecting a fallback node.\")\n",
    "        node_indices = torch.tensor([0])\n",
    "\n",
    "    max_index = graph.x.size(0) - 1\n",
    "    if node_indices.max().item() > max_index:\n",
    "        raise ValueError(\n",
    "            f\"🚨 Error: Selected node index {node_indices.max().item()} exceeds \"\n",
    "            f\"maximum valid index {max_index}.\"\n",
    "        )\n",
    "\n",
    "    # Subset **nodes** (rows), keeping **all features** (columns)\n",
    "    new_x = graph.x[node_indices, :]  # Ensure features remain intact\n",
    "    # Adjust edge_index to reference the selected nodes\n",
    "    new_edge_index = adjust_edge_index(graph.edge_index, node_indices)\n",
    "    subgraph = Data(x=new_x, edge_index=new_edge_index)\n",
    "    subgraph.y = graph.y  # Keep the original label\n",
    "\n",
    "    return subgraph\n",
    "\n",
    "\n",
    "def construct_subgraphs(node_attention_scores, raw_graphs, encoded_graphs, threshold):\n",
    "    \"\"\"\n",
    "    Construct subgraphs based on node attention scores.\n",
    "\n",
    "    Args:\n",
    "        node_attention_scores (torch.Tensor): Node attention weights.\n",
    "        raw_graphs (list): Original graph data.\n",
    "        encoded_graphs (list): Encoded graph data with transformed features.\n",
    "        threshold (float): Threshold for selecting important nodes.\n",
    "\n",
    "    Returns:\n",
    "        list: List of constructed subgraphs.\n",
    "    \"\"\"\n",
    "    subgraphs = []\n",
    "    for raw_graph, encoded_graph in zip(raw_graphs, encoded_graphs):\n",
    "        normalized_scores = normalize_attention_scores(node_attention_scores)\n",
    "        significant_nodes = normalized_scores >= threshold\n",
    "        subgraph = construct_subgraph(encoded_graph, significant_nodes)\n",
    "        subgraphs.append(subgraph)\n",
    "    \n",
    "    return subgraphs\n",
    "\n",
    "def normalize_attention_scores(node_scores):\n",
    "    min_score = node_scores.min()\n",
    "    max_score = node_scores.max()\n",
    "    normalized_scores = (node_scores - min_score) / (max_score - min_score + 1e-5)\n",
    "    return normalized_scores\n",
    "\n",
    "def compute_global_similarity_matrix(graphs):\n",
    "\n",
    "    all_node_embeddings = torch.cat([graph.x for graph in graphs], dim=0)\n",
    "    similarity_matrix = F.cosine_similarity(\n",
    "        all_node_embeddings.unsqueeze(1), all_node_embeddings.unsqueeze(0), dim=2\n",
    "    )\n",
    "    return similarity_matrix\n",
    "\n",
    "# Threshold similarity matrix to create edges\n",
    "def threshold_similarity(similarity_matrix, threshold=0.5):\n",
    "    edge_indices = (similarity_matrix > threshold).nonzero(as_tuple=False)\n",
    "    return edge_indices.t().contiguous()\n",
    "\n",
    "def construct_meta_graph(graphs, similarity_matrix, threshold=0.5):\n",
    "    combined_node_features = torch.cat([graph.x for graph in graphs], dim=0)\n",
    "    \n",
    "    intra_graph_edges = []\n",
    "    offset = 0\n",
    "\n",
    "    for graph in graphs:\n",
    "        intra_edges = graph.edge_index.clone()\n",
    "        intra_edges = intra_edges + offset  # Apply offset correctly\n",
    "        intra_graph_edges.append(intra_edges)\n",
    "        offset += graph.x.shape[0]  # Increment offset properly\n",
    "\n",
    "    intra_graph_edges = torch.cat(intra_graph_edges, dim=1) if intra_graph_edges else torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    inter_graph_edges = threshold_similarity(similarity_matrix, threshold)\n",
    "\n",
    "    # Ensure inter_graph_edges do not exceed node count\n",
    "    valid_mask = (inter_graph_edges[0] < combined_node_features.shape[0]) & (inter_graph_edges[1] < combined_node_features.shape[0])\n",
    "    inter_graph_edges = inter_graph_edges[:, valid_mask]  # Fix shape mismatch\n",
    "\n",
    "\n",
    "    # Ensure indices are valid before concatenation\n",
    "    if intra_graph_edges.numel() > 0:\n",
    "        combined_edges = torch.cat([intra_graph_edges, inter_graph_edges], dim=1)\n",
    "    else:\n",
    "        combined_edges = inter_graph_edges\n",
    "    meta_graph = Data(x=combined_node_features, edge_index=combined_edges)\n",
    "    return meta_graph\n",
    "\n",
    "def adjust_edge_index(edge_index, node_subset):\n",
    "    \"\"\"Remap edge_index so it only contains nodes from node_subset.\"\"\"\n",
    "    node_map = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(node_subset)}\n",
    "    valid_edges = [edge for edge in edge_index.t().tolist() if edge[0] in node_map and edge[1] in node_map]\n",
    "    \n",
    "    if valid_edges:\n",
    "        new_edge_index = torch.tensor([[node_map[src], node_map[dst]] for src, dst in valid_edges], dtype=torch.long).t()\n",
    "        return new_edge_index\n",
    "    else:\n",
    "        return torch.empty((2, 0), dtype=torch.long)  # Return empty edge index\n",
    "\n",
    "def construct_meta_graphs(subgraphs_list):\n",
    "    meta_graphs = []\n",
    "    for i in range(len(subgraphs_list[0])):\n",
    "        similarity_matrix = compute_global_similarity_matrix(\n",
    "            [subgraphs[i] for subgraphs in subgraphs_list]\n",
    "        )\n",
    "        meta_graph = construct_meta_graph(\n",
    "            [subgraphs[i] for subgraphs in subgraphs_list], similarity_matrix, threshold=0.5\n",
    "        )\n",
    "        meta_graph.y = subgraphs_list[0][i].y  # Assign label from one of the subgraphs\n",
    "        meta_graphs.append(meta_graph)\n",
    "    return meta_graphs\n",
    "\n",
    "test_predictions = None   \n",
    "\n",
    "device = torch.device('cpu')  # Force CPU usage\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    global test_predictions\n",
    "    start_time_total = time.time()\n",
    "\n",
    "    thres =  trial.suggest_float(\"thres\", 0.05, 1, step=0.05)\n",
    "    layer_num = trial.suggest_categorical(\"layer_num\", [2,3,4,5,6])\n",
    "    threshold1 = trial.suggest_float(\"threshold1\", 0.1, 1, step=0.1)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    hidden_dim = 32\n",
    "    num_train = trial.suggest_categorical(\"num_train\", [5, 10, 20, 30, 40])\n",
    "    num_epochs = trial.suggest_categorical(\"num_epochs\", [5, 10, 20, 30, 40])\n",
    "    dropout1 = trial.suggest_float(\"dropout1\", 0.1, 0.9, step=0.05)  # Dropout probability\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-0)  # Learning rate (log scale for finer tuning)\n",
    "    train_weak = trial.suggest_categorical(\"train_weak\", [5, 10, 15, 20, 30, 40])\n",
    "    train_middle = trial.suggest_categorical(\"train_middle\", [5, 10, 15, 20, 30, 40])\n",
    "    weight_decay_weak = trial.suggest_loguniform(\"weight_decay_weak\", 1e-5, 1e-2)\n",
    "    lr_weak = trial.suggest_loguniform(\"lr_weak\", 1e-4, 1e-0)\n",
    "    weight_decay_middle = trial.suggest_loguniform(\"weight_decay_middle\", 1e-5, 1e-2)\n",
    "    lr_middle = trial.suggest_loguniform(\"lr_middle\", 1e-4, 1e-0)\n",
    "    batch1 = trial.suggest_categorical(\"batch1\", [8, 16, 32])\n",
    "    best_accuracy = 0\n",
    "    best_hyperparams = None\n",
    "    best_cm = None   \n",
    "    gnn_datasets = {}\n",
    "    for rat_name, data in balanced_datasets.items():\n",
    "        trial_info_balanced = data[\"trial_info\"]\n",
    "        lfp_data_balanced = data[\"lfp_data\"]\n",
    "        gnn_datasets[rat_name] = prepare_gnn_dataset(lfp_data_balanced, trial_info_balanced)\n",
    "\n",
    "    \n",
    "    dataset1 = gnn_datasets[\"superchris\"]\n",
    "    dataset2 = gnn_datasets[\"barat\"]\n",
    "    dataset3 = gnn_datasets[\"stella\"]\n",
    "    dataset4 = gnn_datasets[\"mitt\"]\n",
    "    dataset5 = gnn_datasets[\"buchanan\"]  \n",
    "    \n",
    "    datasets = [dataset1, dataset2, dataset3, dataset4, dataset5]\n",
    "\n",
    "    \n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    \n",
    "    testing = {}\n",
    "    training = {}\n",
    "    validation = {}\n",
    "    \n",
    "    training_labels = {}\n",
    "    testing_labels = {}\n",
    "    validation_labels = {}\n",
    "    \n",
    "    training_edge_indices = {}\n",
    "    testing_edge_indices = {}\n",
    "    validation_edge_indices = {}\n",
    "\n",
    "    adj_matrices = {}\n",
    "    edge_indices = {}\n",
    "    train_dataset = {}\n",
    "    test_datasett = {}\n",
    "    validation_datasett = {}\n",
    "    accuracies = []\n",
    "    test_predictions_all = []\n",
    "    test_labels_all = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    dataset_size = len(dataset1)  # Ensure we're using the right dataset size\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    indices = list(range(dataset_size))  # Ensure indices cover all sample\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(indices)):\n",
    "        print(f\"Starting Fold {fold + 1} - Time: {time.time() - start_time_total:.4f} seconds\")        \n",
    "        print(f\"Fold {fold + 1}:\")\n",
    "        train_idx, validation_idx = train_test_split(train_idx, test_size=0.2, random_state=1)\n",
    "\n",
    "   \n",
    "        for i, dataset in enumerate(datasets):\n",
    "            data_lfp = np.array([data.x.numpy().T for data in dataset])  # Shape: (num_samples, 400, 21)\n",
    "            data_trial = np.array([data.y.item() for data in dataset])  # Labels\n",
    "            testing[i] = data_lfp[test_idx]\n",
    "            training[i] = data_lfp[train_idx]\n",
    "            validation[i] = data_lfp[validation_idx]\n",
    "            \n",
    "            training_labels[i] = data_trial[train_idx]\n",
    "            testing_labels[i] = data_trial[test_idx]\n",
    "            validation_labels[i] = data_trial[validation_idx]\n",
    "            \n",
    "            training_labels[i] = torch.tensor(training_labels[i], dtype=torch.long)\n",
    "            testing_labels[i] = torch.tensor(testing_labels[i], dtype=torch.long)\n",
    "            validation_labels[i] = torch.tensor(validation_labels[i], dtype=torch.long)  # Not used in training\n",
    "        \n",
    "            # Preserve corresponding edge_index for each sample\n",
    "            training_edge_indices[i] = [dataset[j].edge_index for j in train_idx]\n",
    "            testing_edge_indices[i] = [dataset[j].edge_index for j in test_idx]\n",
    "            validation_edge_indices[i] = [dataset[j].edge_index for j in validation_idx]  # Not used in training\n",
    "\n",
    "            \n",
    "            # Define a sample \"data\" object to be used in the model initialization\n",
    "            sample_data = Data(x=training[i][0].T, edge_index=training_edge_indices[i][0])  # Use first training sample\n",
    "            num_classes = len(torch.unique(torch.tensor(data_trial, dtype=torch.long)))\n",
    "            train_dataset[i] = []\n",
    "            for sample in range(training[i].shape[0]):\n",
    "                edge_index = training_edge_indices[i][sample]\n",
    "            \n",
    "                if isinstance(edge_index, list):  \n",
    "                    edge_index = torch.stack(edge_index, dim=1)  # Convert list to tensor\n",
    "            \n",
    "                train_dataset[i].append(Data(x=training[i][sample].T, edge_index=edge_index))\n",
    "    \n",
    "            # Create test and validation datasets in the same format as train_dataset\n",
    "            test_datasett[i] = []\n",
    "            for sample in range(testing[i].shape[0]):\n",
    "                edge_index = testing_edge_indices[i][sample]\n",
    "            \n",
    "                if isinstance(edge_index, list):  \n",
    "                    edge_index = torch.stack(edge_index, dim=1)  # Convert list to tensor\n",
    "            \n",
    "                test_datasett[i].append(Data(x=testing[i][sample].T, edge_index=edge_index))\n",
    "            \n",
    "            validation_datasett[i] = []\n",
    "            for sample in range(validation[i].shape[0]):\n",
    "                edge_index = validation_edge_indices[i][sample]\n",
    "            \n",
    "                if isinstance(edge_index, list):  \n",
    "                    edge_index = torch.stack(edge_index, dim=1)  # Convert list to tensor\n",
    "            \n",
    "                validation_datasett[i].append(Data(x=validation[i][sample].T, edge_index=edge_index))   \n",
    "            training_edge_indices[i] = adjust_edge_index(torch.cat(training_edge_indices[i], dim=1), train_idx)\n",
    "            testing_edge_indices[i] = adjust_edge_index(torch.cat(testing_edge_indices[i], dim=1), test_idx)  \n",
    "            adj_mat = np.mean([np.corrcoef(data_lfp[sample].T) for sample in range(data_lfp.shape[0])], axis=0)\n",
    "            adj_mat[adj_mat < thres] = 0\n",
    "            adj_mat[adj_mat == 1] = 0\n",
    "            adj_mat[adj_mat > 0] = 1\n",
    "            \n",
    "            # Convert to PyTorch tensor\n",
    "            adj_matrices[i] = torch.tensor(adj_mat, dtype=torch.float)\n",
    "            print(adj_matrices[i])\n",
    "            # Create edge_index from adjacency matrix\n",
    "            edge_indices[i] = adj_matrices[i].nonzero().t().contiguous()\n",
    " \n",
    "            training[i], testing[i], validation[i] = normalize_features(training[i], testing[i], validation[i]) \n",
    "           \n",
    "        models = {i: [] for i in range(len(datasets))} \n",
    "        latent_rep = {i: [] for i in range(len(datasets))}\n",
    "        quality_vec = {i: [] for i in range(len(datasets))}\n",
    "        final_emb = {}\n",
    "        test_emb = {}\n",
    "        validation_emb = {}\n",
    "    \n",
    "        torch.manual_seed(1)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for i, dataset in enumerate(datasets):            \n",
    "            sample_data = training[i]  # First sample\n",
    "            num_nodes = sample_data.shape[2]\n",
    "            num_node_features = sample_data.shape[1]\n",
    "            model = Weaker_First(num_nodes, num_node_features, num_classes).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr_weak, weight_decay=weight_decay_weak)\n",
    "            model.train()  \n",
    "            start_time = time.time()\n",
    "            for _ in range(train_weak):\n",
    "                train_loss_compute = compute_loss(model, training[i], edge_indices[i])\n",
    "                training_softprob = train_loss_compute[0]\n",
    "                training_softprob = training_softprob.clone().detach().requires_grad_(True)\n",
    "                optimizer.zero_grad()\n",
    "                training_loss = F.nll_loss(training_softprob, training_labels[i]) \n",
    "                training_loss.backward()\n",
    "                optimizer.step()\n",
    "                train_acc = model_acc(model, training[i], edge_indices[i], training_labels[i])\n",
    "            curr_feat = train_loss_compute[1]\n",
    "            curr_latent = np.array(train_loss_compute[1])\n",
    "            print(f\"weakfirst time {time.time() - start_time:.4f} seconds\")        \n",
    "            models[i].append(model)\n",
    "            latent_rep[i].append(curr_latent)\n",
    "            weights = torch.ones(training[i].shape[0])\n",
    "            weights = nn.functional.normalize(weights, p=2, dim=0)         \n",
    "            err_rate = weight_loss(model, training[i], weights, edge_indices[i], training_labels[i])\n",
    "            quality_vec[i].append(quality_update(err_rate))\n",
    "            start_time = time.time()            \n",
    "            for _ in range(layer_num - 1):\n",
    "                model = Weaker_Middle(256, num_classes).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr_middle, weight_decay=weight_decay_middle)\n",
    "                model.train()\n",
    "                training[i] = np.array(curr_feat) \n",
    "                for epoch in range(train_middle):\n",
    "                    train_loss_compute = compute_loss(model, training[i], edge_indices[i])\n",
    "                    training_softprob = train_loss_compute[0]\n",
    "                    training_softprob = training_softprob.clone().detach().requires_grad_(True)\n",
    "                    optimizer.zero_grad()\n",
    "                    training_loss = F.nll_loss(training_softprob, training_labels[i]) \n",
    "                    training_loss.backward()\n",
    "                    optimizer.step()                    \n",
    "                    train_acc = model_acc(model, training[i], edge_indices[i], training_labels[i])\n",
    "                    curr_feat = train_loss_compute[1]\n",
    "                models[i].append(model)\n",
    "                curr_latent = np.array(train_loss_compute[1])\n",
    "                latent_rep[i].append(curr_latent)\n",
    "                err_rate = weight_loss(model, training[i], weights, edge_indices[i], training_labels[i])\n",
    "                quality_vec[i].append(quality_update(err_rate))             \n",
    "                weights = weight_update(model, err_rate, training[i], weights, edge_indices[i], training_labels[i])                \n",
    "            final_emb[i] = torch.zeros_like(torch.tensor(latent_rep[i][0])).to(device)\n",
    "            for j in range(layer_num):\n",
    "                final_emb[i] += quality_vec[i][j] * torch.tensor(latent_rep[i][j]).to(device)\n",
    "    \n",
    "            # Create training dataset\n",
    "            train_tensor = torch.tensor(training_labels[i], dtype=torch.float32).view(-1, 1)\n",
    "            dataset = TensorDataset(final_emb[i], train_tensor)\n",
    "            train_loader = TorchDataLoader(dataset, batch_size=batch1, shuffle=True)        \n",
    "            test_latent_rep = []         \n",
    "            for j in range(layer_num):\n",
    "                model = models[i][j]\n",
    "                test_loss_compute = compute_loss(model, testing[i], edge_indices[i])\n",
    "                test_latent_rep.append(np.array(test_loss_compute[1]))\n",
    "                testing[i] = np.array(test_loss_compute[1])\n",
    "            test_emb[i] = torch.zeros_like(torch.tensor(test_latent_rep[0])).to(device)          \n",
    "            for j in range(layer_num):\n",
    "                test_emb[i] += quality_vec[i][j] * torch.tensor(test_latent_rep[j]).to(device)\n",
    "            \n",
    "            # Create test dataset\n",
    "            test_tensor = torch.tensor(testing_labels[i], dtype=torch.float32).view(-1, 1)\n",
    "            test_dataset = TensorDataset(test_emb[i], test_tensor)\n",
    "            test_loader = TorchDataLoader(test_dataset, batch_size=batch1, shuffle=True)\n",
    "            validation_latent_rep = []\n",
    "            \n",
    "            for j in range(layer_num):\n",
    "                model = models[i][j]\n",
    "                validation_loss_compute = compute_loss(model, validation[i], edge_indices[i])\n",
    "                validation_latent_rep.append(np.array(validation_loss_compute[1]))\n",
    "                validation[i] = np.array(validation_loss_compute[1])\n",
    "            validation_emb[i] = torch.zeros_like(torch.tensor(validation_latent_rep[0])).to(device)\n",
    "            for j in range(layer_num):\n",
    "                validation_emb[i] += quality_vec[i][j] * torch.tensor(validation_latent_rep[j]).to(device)\n",
    "            \n",
    "            # Create validation dataset\n",
    "            validation_tensor = torch.tensor(validation_labels[i], dtype=torch.float32).view(-1, 1)\n",
    "            validation_dataset = TensorDataset(validation_emb[i], validation_tensor)\n",
    "            validation_loader = TorchDataLoader(validation_dataset, batch_size=batch1, shuffle=True)\n",
    "         \n",
    "        train_subgraphs = []\n",
    "        val_subgraphs = []\n",
    "        test_subgraphs = []\n",
    "        train_subgraphs = []\n",
    "        test_subgraphs = []\n",
    "        validation_subgraphs = [] \n",
    "        for i in range(len(datasets)):\n",
    "            encoded_train_data_list = []\n",
    "            encoded_test_data_list = []\n",
    "            encoded_validation_data_list = []\n",
    "        \n",
    "            # Encode Train Dataset\n",
    "            for sample in range(len(train_dataset[i])):  # Iterate over Data objects\n",
    "                node_features = final_emb[i][sample]  # Ensure shape [num_nodes, num_features]\n",
    "                encoded_data = Data(x=node_features, edge_index=train_dataset[i][sample].edge_index)\n",
    "                encoded_train_data_list.append(encoded_data)\n",
    "        \n",
    "            # Encode Test Dataset\n",
    "            for sample in range(len(test_datasett[i])):  # Iterate over Data objects\n",
    "                node_features = test_emb[i][sample]  # Ensure shape [num_nodes, num_features]\n",
    "                encoded_data = Data(x=node_features, edge_index=test_datasett[i][sample].edge_index)\n",
    "                encoded_test_data_list.append(encoded_data)\n",
    "        \n",
    "            # Encode Validation Dataset\n",
    "            for sample in range(len(validation_datasett[i])):  # Iterate over Data objects\n",
    "                node_features = validation_emb[i][sample]  # Ensure shape [num_nodes, num_features]\n",
    "                encoded_data = Data(x=node_features, edge_index=validation_datasett[i][sample].edge_index)\n",
    "                encoded_validation_data_list.append(encoded_data)\n",
    "\n",
    "            attention_scores = torch.zeros_like(models[i][0].attention_scores)\n",
    "            \n",
    "            # Iterate over each model layer\n",
    "            for j in range(layer_num):\n",
    "                # Retrieve the attention scores from the current model\n",
    "                current_attention_scores = models[i][j].attention_scores\n",
    "                # Retrieve the quality score for the current model\n",
    "                quality_score = quality_vec[i][j]                \n",
    "                # Accumulate the weighted attention scores\n",
    "                attention_scores += quality_score * current_attention_scores\n",
    "            node_attention_scores_train = compute_node_attention_scores(\n",
    "                attention_scores, train_dataset[i][0].edge_index, train_dataset[i][0].x.shape[0]\n",
    "            )\n",
    "        \n",
    "            # Construct subgraphs for training set\n",
    "            train_subgraphs.append(\n",
    "                construct_subgraphs(node_attention_scores_train, train_dataset[i], encoded_train_data_list, threshold=threshold1)\n",
    "            )\n",
    "        \n",
    "            # Compute node attention scores from extracted attention weights (Test)\n",
    "            node_attention_scores_test = compute_node_attention_scores(\n",
    "                attention_scores, test_datasett[i][0].edge_index, test_datasett[i][0].x.shape[0]\n",
    "            )\n",
    "    \n",
    "            # Construct subgraphs for test set\n",
    "            test_subgraphs.append(\n",
    "                construct_subgraphs(node_attention_scores_test, test_datasett[i], encoded_test_data_list, threshold=threshold1)\n",
    "            )\n",
    "        \n",
    "            # Compute node attention scores from extracted attention weights (Validation)\n",
    "            node_attention_scores_validation = compute_node_attention_scores(\n",
    "                attention_scores, validation_datasett[i][0].edge_index, validation_datasett[i][0].x.shape[0]\n",
    "            )\n",
    "        \n",
    "            # Construct subgraphs for validation set\n",
    "            validation_subgraphs.append(\n",
    "                construct_subgraphs(node_attention_scores_validation, validation_datasett[i], encoded_validation_data_list, threshold=threshold1)\n",
    "            )    \n",
    "    \n",
    "        train_meta_graphs = construct_meta_graphs(train_subgraphs)\n",
    "        val_meta_graphs = construct_meta_graphs(validation_subgraphs)\n",
    "        test_meta_graphs = construct_meta_graphs(test_subgraphs)       \n",
    "        \n",
    "        # Assign labels to meta-graphs explicitly\n",
    "        for i, graph in enumerate(train_meta_graphs):\n",
    "            graph.y = torch.tensor(training_labels[0][i], dtype=torch.long)  # Assign correct labels\n",
    "        \n",
    "        for i, graph in enumerate(val_meta_graphs):\n",
    "            graph.y = torch.tensor(validation_labels[0][i], dtype=torch.long)  # Assign validation labels\n",
    "        \n",
    "        for i, graph in enumerate(test_meta_graphs):\n",
    "            graph.y = torch.tensor(testing_labels[0][i], dtype=torch.long)  # Assign test labels\n",
    "        \n",
    "\n",
    "\n",
    "        for graph in train_meta_graphs:\n",
    "            if graph.y.dim() == 0:  # If the label is scalar\n",
    "                graph.y = graph.y.unsqueeze(0)  # Convert to tensor of shape [1]\n",
    "        for graph in val_meta_graphs:\n",
    "            if graph.y.dim() == 0:\n",
    "                graph.y = graph.y.unsqueeze(0)\n",
    "        \n",
    "        for graph in test_meta_graphs:\n",
    "            if graph.y.dim() == 0:\n",
    "                graph.y = graph.y.unsqueeze(0)\n",
    "        \n",
    "        from torch_geometric.loader import DataLoader  # Use PyG's DataLoader\n",
    "        \n",
    "        train_loader = DataLoader(train_meta_graphs, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_meta_graphs, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_meta_graphs, batch_size=batch_size, shuffle=False)       \n",
    "        final_model = GraphLevelPredictor(input_dim=128, hidden_dim=hidden_dim, output_dim=5, dropout= dropout1)\n",
    "    \n",
    "        final_model.load_state_dict(final_model.state_dict())\n",
    "        torch.manual_seed(1)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False \n",
    "\n",
    "    \n",
    "        set_weights(final_model)\n",
    "        optimizer = torch.optim.Adam(final_model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_model_state = None\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_model_state = None\n",
    "        \n",
    "        # Initialize accuracy tracking lists\n",
    "    \n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_train):\n",
    "            final_model.train()\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = final_model(batch)\n",
    "                loss = criterion(out, batch.y.view(-1).long())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item() \n",
    "        correct_val, total_val = 0, 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                out = final_model(batch)\n",
    "                pred = torch.argmax(out, dim=1)\n",
    "                correct_val += (pred == batch.y.view(-1).long()).sum().item()\n",
    "                total_val += batch.y.size(0)\n",
    "    \n",
    "        val_accuracy = correct_val / total_val\n",
    "        val_accuracies.append(val_accuracy)\n",
    "    \n",
    "        print(f\"Fold {fold + 1} Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Instead of test accuracy, return average validation accuracy for Optuna\n",
    "    final_val_accuracy = np.mean(val_accuracies)\n",
    "    print(f\"Final Average Validation Accuracy across folds: {final_val_accuracy:.4f}\")\n",
    "    return final_val_accuracy\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=1))\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Best trial\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_best_model(best_params, gnn_datasets, num_classes=5):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from torch_geometric.loader import DataLoader\n",
    "    print(\"\\n🔍 Starting final evaluation using best hyperparameters...\\n\")\n",
    "\n",
    "    # Pick first fold as test set\n",
    "    dataset_size = len(gnn_datasets[\"superchris\"])\n",
    "    indices = list(range(dataset_size))\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    train_idx, test_idx = next(kf.split(indices))\n",
    "\n",
    "    # Prepare full train+val and test datasets (adapt based on your prepare_gnn_dataset logic)\n",
    "    datasets = [gnn_datasets[name] for name in [\"superchris\", \"barat\", \"stella\", \"mitt\", \"buchanan\"]]\n",
    "    \n",
    "    training = {}\n",
    "    testing = {}\n",
    "    train_dataset = {}\n",
    "    test_datasett = {}\n",
    "    training_labels = {}\n",
    "    testing_labels = {}\n",
    "    training_edge_indices = {}\n",
    "    testing_edge_indices = {}\n",
    "    adj_matrices = {}\n",
    "    edge_indices = {}\n",
    "\n",
    "    lr_weak = best_params[\"lr_weak\"]\n",
    "    weight_decay_weak = best_params[\"weight_decay_weak\"]\n",
    "    lr_middle = best_params[\"lr_middle\"]\n",
    "    weight_decay_middle = best_params[\"weight_decay_middle\"]\n",
    "    train_weak = best_params[\"train_weak\"]\n",
    "    train_middle = best_params[\"train_middle\"]\n",
    "    layer_num = best_params[\"layer_num\"]\n",
    "    threshold1 = best_params[\"threshold1\"]\n",
    "    batch1 = best_params[\"batch1\"]\n",
    "\n",
    "\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        data_lfp = np.array([data.x.numpy().T for data in dataset])  # Shape: (num_samples, 400, 21)\n",
    "        data_trial = np.array([data.y.item() for data in dataset])  # Labels\n",
    "        testing[i] = data_lfp[test_idx]\n",
    "        training[i] = data_lfp[train_idx]\n",
    "        \n",
    "        training_labels[i] = data_trial[train_idx]\n",
    "        testing_labels[i] = data_trial[test_idx]\n",
    "        \n",
    "        training_labels[i] = torch.tensor(training_labels[i], dtype=torch.long)\n",
    "        testing_labels[i] = torch.tensor(testing_labels[i], dtype=torch.long)\n",
    "        # Preserve corresponding edge_index for each sample\n",
    "        training_edge_indices[i] = [dataset[j].edge_index for j in train_idx]\n",
    "        testing_edge_indices[i] = [dataset[j].edge_index for j in test_idx]\n",
    "\n",
    "        # Define a sample \"data\" object to be used in the model initialization\n",
    "        sample_data = Data(x=training[i][0].T, edge_index=training_edge_indices[i][0])  # Use first training sample\n",
    "        num_classes = len(torch.unique(torch.tensor(data_trial, dtype=torch.long)))\n",
    "        train_dataset[i] = []\n",
    "        for sample in range(training[i].shape[0]):\n",
    "            edge_index = training_edge_indices[i][sample]\n",
    "        \n",
    "            if isinstance(edge_index, list):  \n",
    "                edge_index = torch.stack(edge_index, dim=1)  # Convert list to tensor\n",
    "        \n",
    "            train_dataset[i].append(Data(x=training[i][sample].T, edge_index=edge_index))\n",
    "\n",
    "        test_datasett[i] = []\n",
    "        for sample in range(testing[i].shape[0]):\n",
    "            edge_index = testing_edge_indices[i][sample]\n",
    "        \n",
    "            if isinstance(edge_index, list):  \n",
    "                edge_index = torch.stack(edge_index, dim=1)  # Convert list to tensor\n",
    "        \n",
    "            test_datasett[i].append(Data(x=testing[i][sample].T, edge_index=edge_index))\n",
    "\n",
    "        training_edge_indices[i] = adjust_edge_index(torch.cat(training_edge_indices[i], dim=1), train_idx)\n",
    "        testing_edge_indices[i] = adjust_edge_index(torch.cat(testing_edge_indices[i], dim=1), test_idx)  \n",
    "        adj_mat = np.mean([np.corrcoef(data_lfp[sample].T) for sample in range(data_lfp.shape[0])], axis=0)\n",
    "        adj_mat[adj_mat < thres] = 0\n",
    "        adj_mat[adj_mat == 1] = 0\n",
    "        adj_mat[adj_mat > 0] = 1\n",
    "        \n",
    "        # Convert to PyTorch tensor\n",
    "        adj_matrices[i] = torch.tensor(adj_mat, dtype=torch.float)\n",
    "        \n",
    "        # Create edge_index from adjacency matrix\n",
    "        edge_indices[i] = adj_matrices[i].nonzero().t().contiguous()\n",
    "\n",
    "        training[i], testing[i] = normalize_features(training[i], testing[i]) \n",
    "       \n",
    "    models = {i: [] for i in range(len(datasets))} \n",
    "    latent_rep = {i: [] for i in range(len(datasets))}\n",
    "    quality_vec = {i: [] for i in range(len(datasets))}\n",
    "    final_emb = {}\n",
    "    test_emb = {}\n",
    "\n",
    "    torch.manual_seed(1)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i, dataset in enumerate(datasets):            \n",
    "        sample_data = training[i]  # First sample\n",
    "        num_nodes = sample_data.shape[2]\n",
    "        num_node_features = sample_data.shape[1]\n",
    "        model = Weaker_First(num_nodes, num_node_features, num_classes).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr_weak, weight_decay=weight_decay_weak)\n",
    "        model.train()  \n",
    "        start_time = time.time()\n",
    "        for _ in range(train_weak):\n",
    "            train_loss_compute = compute_loss(model, training[i], edge_indices[i])\n",
    "            training_softprob = train_loss_compute[0]\n",
    "            training_softprob = training_softprob.clone().detach().requires_grad_(True)\n",
    "            optimizer.zero_grad()\n",
    "            training_loss = F.nll_loss(training_softprob, training_labels[i]) \n",
    "            training_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_acc = model_acc(model, training[i], edge_indices[i], training_labels[i])\n",
    "        curr_feat = train_loss_compute[1]\n",
    "        curr_latent = np.array(train_loss_compute[1])\n",
    "        print(f\"weakfirst time {time.time() - start_time:.4f} seconds\")        \n",
    "        models[i].append(model)\n",
    "        latent_rep[i].append(curr_latent)\n",
    "        weights = torch.ones(training[i].shape[0])\n",
    "        weights = nn.functional.normalize(weights, p=2, dim=0)         \n",
    "        err_rate = weight_loss(model, training[i], weights, edge_indices[i], training_labels[i])\n",
    "        quality_vec[i].append(quality_update(err_rate))\n",
    "        start_time = time.time()            \n",
    "        for _ in range(layer_num - 1):\n",
    "            model = Weaker_Middle(256, num_classes).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr_middle, weight_decay=weight_decay_middle)\n",
    "            model.train()\n",
    "            training[i] = np.array(curr_feat) \n",
    "            for epoch in range(train_middle):\n",
    "                train_loss_compute = compute_loss(model, training[i], edge_indices[i])\n",
    "                training_softprob = train_loss_compute[0]\n",
    "                training_softprob = training_softprob.clone().detach().requires_grad_(True)\n",
    "                optimizer.zero_grad()\n",
    "                training_loss = F.nll_loss(training_softprob, training_labels[i]) \n",
    "                training_loss.backward()\n",
    "                optimizer.step()                    \n",
    "                train_acc = model_acc(model, training[i], edge_indices[i], training_labels[i])\n",
    "                curr_feat = train_loss_compute[1]\n",
    "            models[i].append(model)\n",
    "            curr_latent = np.array(train_loss_compute[1])\n",
    "            latent_rep[i].append(curr_latent)\n",
    "            err_rate = weight_loss(model, training[i], weights, edge_indices[i], training_labels[i])\n",
    "            quality_vec[i].append(quality_update(err_rate))             \n",
    "            weights = weight_update(model, err_rate, training[i], weights, edge_indices[i], training_labels[i])                \n",
    "        final_emb[i] = torch.zeros_like(torch.tensor(latent_rep[i][0])).to(device)\n",
    "        for j in range(layer_num):\n",
    "            final_emb[i] += quality_vec[i][j] * torch.tensor(latent_rep[i][j]).to(device)\n",
    "\n",
    "        # Create training dataset\n",
    "        train_tensor = torch.tensor(training_labels[i], dtype=torch.float32).view(-1, 1)\n",
    "        dataset = TensorDataset(final_emb[i], train_tensor)\n",
    "        train_loader = TorchDataLoader(dataset, batch_size=batch1, shuffle=True)        \n",
    "        test_latent_rep = []         \n",
    "        for j in range(layer_num):\n",
    "            model = models[i][j]\n",
    "            test_loss_compute = compute_loss(model, testing[i], edge_indices[i])\n",
    "            test_latent_rep.append(np.array(test_loss_compute[1]))\n",
    "            testing[i] = np.array(test_loss_compute[1])\n",
    "        test_emb[i] = torch.zeros_like(torch.tensor(test_latent_rep[0])).to(device)          \n",
    "        for j in range(layer_num):\n",
    "            test_emb[i] += quality_vec[i][j] * torch.tensor(test_latent_rep[j]).to(device)\n",
    "        \n",
    "        # Create test dataset\n",
    "        test_tensor = torch.tensor(testing_labels[i], dtype=torch.float32).view(-1, 1)\n",
    "        test_dataset = TensorDataset(test_emb[i], test_tensor)\n",
    "        test_loader = TorchDataLoader(test_dataset, batch_size=batch1, shuffle=True)\n",
    "\n",
    "    train_subgraphs = []\n",
    "    test_subgraphs = []\n",
    "    train_subgraphs = []\n",
    "    test_subgraphs = []\n",
    "    for i in range(len(datasets)):\n",
    "        encoded_train_data_list = []\n",
    "        encoded_test_data_list = []\n",
    "    \n",
    "        # Encode Train Dataset\n",
    "        for sample in range(len(train_dataset[i])):  # Iterate over Data objects\n",
    "            node_features = final_emb[i][sample]  # Ensure shape [num_nodes, num_features]\n",
    "            encoded_data = Data(x=node_features, edge_index=train_dataset[i][sample].edge_index)\n",
    "            encoded_train_data_list.append(encoded_data)\n",
    "    \n",
    "        # Encode Test Dataset\n",
    "        for sample in range(len(test_datasett[i])):  # Iterate over Data objects\n",
    "            node_features = test_emb[i][sample]  # Ensure shape [num_nodes, num_features]\n",
    "            encoded_data = Data(x=node_features, edge_index=test_datasett[i][sample].edge_index)\n",
    "            encoded_test_data_list.append(encoded_data)\n",
    "\n",
    "\n",
    "        attention_scores = torch.zeros_like(models[i][0].attention_scores)\n",
    "        \n",
    "        # Iterate over each model layer\n",
    "        for j in range(layer_num):\n",
    "            # Retrieve the attention scores from the current model\n",
    "            current_attention_scores = models[i][j].attention_scores\n",
    "            # Retrieve the quality score for the current model\n",
    "            quality_score = quality_vec[i][j]                \n",
    "            # Accumulate the weighted attention scores\n",
    "            attention_scores += quality_score * current_attention_scores\n",
    "        node_attention_scores_train = compute_node_attention_scores(\n",
    "            attention_scores, train_dataset[i][0].edge_index, train_dataset[i][0].x.shape[0]\n",
    "        )\n",
    "    \n",
    "        # Construct subgraphs for training set\n",
    "        train_subgraphs.append(\n",
    "            construct_subgraphs(node_attention_scores_train, train_dataset[i], encoded_train_data_list, threshold=threshold1)\n",
    "        )\n",
    "    \n",
    "        # Compute node attention scores from extracted attention weights (Test)\n",
    "        node_attention_scores_test = compute_node_attention_scores(\n",
    "            attention_scores, test_datasett[i][0].edge_index, test_datasett[i][0].x.shape[0]\n",
    "        )\n",
    "\n",
    "        # Construct subgraphs for test set\n",
    "        test_subgraphs.append(\n",
    "            construct_subgraphs(node_attention_scores_test, test_datasett[i], encoded_test_data_list, threshold=threshold1)\n",
    "        )\n",
    "    \n",
    "\n",
    "    train_meta_graphs = construct_meta_graphs(train_subgraphs)\n",
    "    test_meta_graphs = construct_meta_graphs(test_subgraphs)       \n",
    "    \n",
    "    # Assign labels to meta-graphs explicitly\n",
    "    for i, graph in enumerate(train_meta_graphs):\n",
    "        graph.y = torch.tensor(training_labels[0][i], dtype=torch.long)  # Assign correct labels\n",
    "    \n",
    "    for i, graph in enumerate(test_meta_graphs):\n",
    "        graph.y = torch.tensor(testing_labels[0][i], dtype=torch.long)  # Assign test labels\n",
    "    \n",
    "\n",
    "\n",
    "    for graph in train_meta_graphs:\n",
    "        if graph.y.dim() == 0:  # If the label is scalar\n",
    "            graph.y = graph.y.unsqueeze(0)  # Convert to tensor of shape [1]\n",
    "    \n",
    "    for graph in test_meta_graphs:\n",
    "        if graph.y.dim() == 0:\n",
    "            graph.y = graph.y.unsqueeze(0)\n",
    "    \n",
    "    from torch_geometric.loader import DataLoader  # Use PyG's DataLoader\n",
    "\n",
    "    train_loader = DataLoader(train_meta_graphs, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "    test_loader = DataLoader(test_meta_graphs, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # Initialize model\n",
    "    model = GraphLevelPredictor(\n",
    "        input_dim=128,\n",
    "        hidden_dim=32,  # Adjust if tunable\n",
    "        output_dim=num_classes,\n",
    "        dropout=best_params[\"dropout1\"]\n",
    "    ).to(device)\n",
    "\n",
    "    set_weights(model)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=best_params[\"lr\"], \n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train on train+val\n",
    "    model.train()\n",
    "    for epoch in range(best_params[\"num_train\"]):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = criterion(out, batch.y.view(-1).long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            out = model(batch)\n",
    "            preds = torch.argmax(out, dim=1)\n",
    "            correct += (preds == batch.y.view(-1)).sum().item()\n",
    "            total += batch.y.size(0)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(batch.y.view(-1).tolist())\n",
    "\n",
    "    final_test_accuracy = correct / total\n",
    "    print(f\"\\n✅ Final Test Accuracy (on held-out fold): {final_test_accuracy:.4f}\")\n",
    "    return final_test_accuracy, all_preds, all_labels\n",
    "\n",
    "\n",
    "final_accuracy, preds, labels = evaluate_best_model(best_params, gnn_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd7871-0060-4944-9832-ca751df2f9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
